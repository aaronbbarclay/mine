{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Macbeth_learn_maskrcnn_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronbbarclay/mine/blob/master/Macbeth_learn_maskrcnn_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19zafzl1Ptq2",
        "colab_type": "text"
      },
      "source": [
        "Using mask RCNN to train macbeth detection\n",
        "\n",
        "https://github.com/matterport/Mask_RCNN/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EMcC41nfjCd",
        "colab_type": "code",
        "outputId": "fab676df-fc97-4874-e555-30174f4b3b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.13.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: opensource@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: protobuf, termcolor, absl-py, wheel, keras-applications, grpcio, numpy, astor, tensorflow-estimator, gast, tensorboard, keras-preprocessing, six\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0cXN-OOKkVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dR8HxrO_oI7",
        "colab_type": "code",
        "outputId": "7a845203-d7f9-4256-a5a2-ab0031990840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        }
      },
      "source": [
        "%cd /root/\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "#!git clone --quiet https://github.com/matterport/Mask_RCNN/\n",
        "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-tk is already the newest version (2.7.15~rc1-1).\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-olefile\n",
            "  python-pkg-resources python-six python-webencodings\n",
            "Suggested packages:\n",
            "  python-genshi python-lxml-dbg python-lxml-doc python-pil-doc python-pil-dbg\n",
            "  python-setuptools\n",
            "The following NEW packages will be installed:\n",
            "  python-bs4 python-chardet python-html5lib python-lxml python-olefile\n",
            "  python-pil python-pkg-resources python-six python-webencodings\n",
            "0 upgraded, 9 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 1,818 kB of archives.\n",
            "After this operation, 7,688 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-bs4 all 4.6.0-1 [67.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-chardet all 3.0.4-1 [80.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-webencodings all 0.5-2 [10.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-html5lib all 0.999999999-1 [83.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-lxml amd64 4.2.1-1ubuntu0.1 [1,075 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-olefile all 0.45.1-1 [33.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pil amd64 5.1.0-1 [328 kB]\n",
            "Fetched 1,818 kB in 1s (1,265 kB/s)\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "\u001b[K     |████████████████████████████████| 993kB 5.0MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEJh-6EXuryi",
        "colab_type": "code",
        "outputId": "6afe357f-ed4e-4005-b8f3-5cd58e02b900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!mkdir -p /content/drive/\n",
        "\n",
        "#Mount Google Drive as folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONvvLz1uLM-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3b93d12c-d245-4c89-aee0-b76a4213d925"
      },
      "source": [
        "%cd /root\n",
        "%ls\n",
        "!mv Mask_RCNN/ Mask_RCNN_old/"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "\u001b[0m\u001b[01;34mMask_RCNN\u001b[0m/  \u001b[01;34mmodels\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz8xh55NKnbu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df9e66df-22a7-4b8e-a810-a07cfd92725f"
      },
      "source": [
        "!mkdir -p /content/drive/My\\ Drive/MachineLearning/tools\n",
        "%cd /content/drive/My\\ Drive/MachineLearning/tools/\n",
        "!git clone --quiet https://github.com/matterport/Mask_RCNN/"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MachineLearning/tools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES-su46RRN_X",
        "colab_type": "code",
        "outputId": "ba5ea5b6-64bb-4767-9460-201c26aee4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "######\n",
        "# Only do this once as the trainied model is stored in this file. -- now stored in google drive\n",
        "######\n",
        "\n",
        "#!mkdir -p /root/data\n",
        "#%cd /content/drive/My\\ Drive/MachineLearning/projects/macbethIdentify_v1\n",
        "#!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1\n",
            "--2019-06-08 11:45:50--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190608%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190608T114550Z&X-Amz-Expires=300&X-Amz-Signature=e053c00356fd0bdc03a9c9e4b5cf4b32d3681a9ba99b8dd506911e16054a48a7&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-06-08 11:45:50--  https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190608%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190608T114550Z&X-Amz-Expires=300&X-Amz-Signature=e053c00356fd0bdc03a9c9e4b5cf4b32d3681a9ba99b8dd506911e16054a48a7&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.10.11\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.10.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257557808 (246M) [application/octet-stream]\n",
            "Saving to: ‘mask_rcnn_coco.h5’\n",
            "\n",
            "mask_rcnn_coco.h5   100%[===================>] 245.63M  63.2MB/s    in 4.1s    \n",
            "\n",
            "2019-06-08 11:45:55 (59.2 MB/s) - ‘mask_rcnn_coco.h5’ saved [257557808/257557808]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSlGKXeL_pzS",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "## This is not needed\n",
        "\n",
        "### Install pycotools\n",
        "##!pip install -q pycocotools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRULdCfrA39Y",
        "colab_type": "code",
        "outputId": "4932e0d4-efaf-4f7d-d0ab-9fcfac959790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# This is not needed\n",
        "\n",
        "# Compile protocol buffers\n",
        "#%cd ~/models/research\n",
        "#!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khuHLMHuMqAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/drive/My Drive/MachineLearning/tools\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NhhQJov6MM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ccfd1f7-9931-486a-ded9-309c6093cb44"
      },
      "source": [
        "%cd /\n",
        "import six\n",
        "import mrcnn"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB5fbc3_piye",
        "colab_type": "code",
        "outputId": "9804c8b2-0f40-4a35-fd33-3d10bf2972b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import os\n",
        "\n",
        "%cd /content/drive/My\\ Drive/\n",
        "%cd MachineLearning/projects/macbethIdentify_v1/data/img/\n",
        "\n",
        "\n",
        "BASEDIR = \"/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1\"\n",
        "print(os.path.isdir(BASEDIR))\n",
        "print(BASEDIR)\n",
        "\n",
        "WEIGHTS_FILE = \"{0}/{1}\".format(BASEDIR, \"mask_rcnn_coco.h5\")\n",
        "LOG_DIR = \"{0}/{1}\".format(BASEDIR, \"LOGS\")\n",
        "imageFilesBasePath = \"{0}/{1}\".format(BASEDIR, \"/data/img\")\n",
        "maskFilesBasePath = \"{0}/{1}\".format(BASEDIR, \"/data/mask_images\")\n",
        "valFilesBasePath = \"{0}/{1}\".format(BASEDIR, \"/data/img_val\")\n",
        "#annotationsFilesBasePath = '/root/data/mine/machineLearning/macbethIdentify/data/macbeth_identify/data/data/annotations/'\n",
        "#labelMapPath = '/root/data/mine/machineLearning/macbethIdentify/data/macbeth_identify/label_map.pbtxt'\n",
        "#testPath = '/root/data/mine/machineLearning/macbethIdentify/data/macbeth_identify/test.txt'\n",
        "#trainPath = '/root/data/mine/machineLearning/macbethIdentify/data/macbeth_identify/train.txt'\n",
        "#valPath = '/root/data/mine/machineLearning/macbethIdentify/data/macbeth_identify/val.txt'\n",
        "\n",
        "print(imageFilesBasePath)\n",
        "print(maskFilesBasePath)\n",
        "print(valFilesBasePath)\n",
        "print(os.path.isdir(imageFilesBasePath))\n",
        "print(os.path.isdir(maskFilesBasePath))\n",
        "print(os.path.isdir(valFilesBasePath))\n",
        "print(os.path.isfile(WEIGHTS_FILE))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/img\n",
            "True\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1//data/img\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1//data/mask_images\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1//data/img_val\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNHVEpJ6g6bU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1516
        },
        "outputId": "8fe2a686-4f26-42bf-8790-928bf0a35336"
      },
      "source": [
        "# Convert rectlabel annotation to maskrcnn json\n",
        "\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "\n",
        "ANNOTATION_DIR = \"{0}/data/annotations\".format(BASEDIR)\n",
        "JSON_DIR = \"{0}/data/annotations_json\".format(BASEDIR)\n",
        "JSON_FILE = \"{0}/via_region_data.json\".format(JSON_DIR)\n",
        "\n",
        "print(os.path.isdir(ANNOTATION_DIR))\n",
        "print(os.path.isdir(JSON_DIR))\n",
        "\n",
        "allAnnotations = glob.glob(ANNOTATION_DIR + \"/*\")\n",
        "\n",
        "print(allAnnotations)\n",
        "\n",
        "class JsonAnnotation:\n",
        "    def __init__(self):\n",
        "        self.filename = None\n",
        "        #self.fileref = \"\"\n",
        "        #self.size = \"\"\n",
        "        #self.base64_img_data = \"\"\n",
        "        #self.file_attributes = {}\n",
        "        self.all_points_x = []\n",
        "        self.all_points_y = []\n",
        "        \n",
        "    def dict1(self):\n",
        "        d = {}\n",
        "        d[\"fileref\"] = \"\"\n",
        "        d[\"size\"] = 23232323\n",
        "        d[\"filename\"] = self.filename\n",
        "        d[\"base64_img_data\"] = \"\"\n",
        "        d[\"file_attributes\"] = {}\n",
        "        d[\"regions\"] = self.createRegions()\n",
        "        \n",
        "        return d\n",
        "    \n",
        "    def createRegions(self):\n",
        "        regions = {0 : {}}\n",
        "        regions[0] = {}\n",
        "        regions[0][\"shape_attributes\"] = {}\n",
        "        regions[0][\"shape_attributes\"][\"name\"] = \"polygon\"\n",
        "        regions[0][\"shape_attributes\"][\"region_attributes\"] = {}\n",
        "        regions[0][\"shape_attributes\"][\"all_points_x\"] = self.all_points_x\n",
        "        regions[0][\"shape_attributes\"][\"all_points_y\"] = self.all_points_y\n",
        "        \n",
        "        return regions      \n",
        "    \n",
        "    def getString(self):\n",
        "        return {self.filename: \"test\"}\n",
        "\n",
        "allJsonAnnotations = {}\n",
        "\n",
        "for a in allAnnotations:\n",
        "    #if not a.count(\"megan_rapinoe_mg_0013\"):\n",
        "    #    continue\n",
        "\n",
        "    print(a)\n",
        "    tree = ET.parse(a)\n",
        "    root = tree.getroot()\n",
        "    #print(root)\n",
        "    #print(root.find(\"folder\").text)\n",
        "    #print(root.find(\"object\").get(\"name\"))\n",
        "    #print()\n",
        "\n",
        "    jsonAnnotation = JsonAnnotation()\n",
        "    jsonAnnotation.filename = root.find(\"filename\").text \n",
        "\n",
        "    _object = root.find(\"object\")\n",
        "    name = _object.find(\"name\").text \n",
        " \n",
        "    for child in _object:\n",
        "        if child.tag != \"polygon\":\n",
        "            continue\n",
        "            \n",
        "        for point in child:\n",
        "            #print(point.tag, \": \", point.text)\n",
        "            if point.tag.count(\"x\"):\n",
        "                jsonAnnotation.all_points_x.append(int(point.text))\n",
        "            else:\n",
        "                jsonAnnotation.all_points_y.append(int(point.text))\n",
        "    \n",
        "    allJsonAnnotations[jsonAnnotation.filename] = jsonAnnotation.dict1()\n",
        "    \n",
        "    \n",
        "with open(JSON_FILE, 'w') as f:\n",
        "    result = json.dump(allJsonAnnotations, f)\n",
        "    \n",
        "#print(result)\n",
        "\n",
        "print(\"writing: \", JSON_FILE)\n",
        "#f = open(JSON_FILE, 'w')\n",
        "#f.write(result)\n",
        "\n",
        "    #annotation = root.tag\n",
        "    #\n",
        "    \n",
        "    #polygon = root.find(\"polygon\")\n",
        "\n",
        "    #print(root.find(\"filename\").text)\n",
        "    #print(polygon)\n",
        "    "
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "['/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/jeff_kolitch_00097-mos.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/kaztest_mg_0015.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/penn_jillette_00106.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/kaztest.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/megan_rapinoe_00023.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/megan_rapinoe_mg_0013.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/megan_rapinoe_mg_0100.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/mg_0006.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/mg_0008.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/p04.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/penn_jillette_00135.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/robert_benmosche_00011.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/rt2012_xboard_00012-mos.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/sg11.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/sg13.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/sg8.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/t-7797r.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/2_associates_00002.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/2013_rt_cover_brian_rogers_00001-mos.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/6associates_bluetest.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/7.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5533_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5545_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/ARMAFC.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bob_amsterdam_00052.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bobby_flay_00217.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bobby_flay_00254.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/brendan_haircut_00001.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bts1.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/catherine_mary_stewart_00074.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/catherine_mary_stewart_00331-mos.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/catherine_mary_stewart_00461.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/deb1.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/girl-with-colorchecker.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/img_0172.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0319_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0341_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0475_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0481_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0750_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1309 2.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1310.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1311.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1312.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1313.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1314.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1315.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1316.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1317.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1318.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1319.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1320.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1321.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1322.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1323.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1324.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1325.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1326.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1327.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1328.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1329.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1330.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1331.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1332.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1333.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1335.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1336.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1337.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1338.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1339.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1340.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1341.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1342.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1343.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1344.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1346.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1347.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1348.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1349.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1334.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5595_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5611_orig.xml', '/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1345.xml']\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/jeff_kolitch_00097-mos.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/kaztest_mg_0015.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/penn_jillette_00106.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/kaztest.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/megan_rapinoe_00023.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/megan_rapinoe_mg_0013.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/megan_rapinoe_mg_0100.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/mg_0006.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/mg_0008.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/p04.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/penn_jillette_00135.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/robert_benmosche_00011.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/rt2012_xboard_00012-mos.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/sg11.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/sg13.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/sg8.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/t-7797r.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/2_associates_00002.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/2013_rt_cover_brian_rogers_00001-mos.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/6associates_bluetest.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/7.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5533_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5545_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/ARMAFC.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bob_amsterdam_00052.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bobby_flay_00217.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bobby_flay_00254.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/brendan_haircut_00001.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/bts1.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/catherine_mary_stewart_00074.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/catherine_mary_stewart_00331-mos.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/catherine_mary_stewart_00461.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/deb1.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/girl-with-colorchecker.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/img_0172.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0319_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0341_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0475_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0481_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_0750_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1309 2.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1310.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1311.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1312.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1313.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1314.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1315.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1316.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1317.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1318.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1319.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1320.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1321.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1322.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1323.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1324.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1325.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1326.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1327.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1328.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1329.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1330.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1331.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1332.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1333.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1335.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1336.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1337.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1338.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1339.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1340.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1341.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1342.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1343.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1344.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1346.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1347.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1348.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1349.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1334.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5595_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/8D5U5611_orig.xml\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations/IMG_1345.xml\n",
            "writing:  /content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1/data/annotations_json/via_region_data.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-0tqfOiySx7",
        "colab": {}
      },
      "source": [
        "def getImageFromDir(basePath=None, filename=None, className=None):\n",
        "    name, ext = filename.split(\".\")\n",
        "    dirContents = os.listdir(basePath)\n",
        "    for f in dirContents:\n",
        "        maskName, maskExtension = f.split('.')\n",
        "        #print(\"Has className: \", f, \" \" , className, \" \", className in f)\n",
        "        if f.startswith(name) and className in f:\n",
        "            #print(os.path.join(basePath, f))\n",
        "            return os.path.join(basePath, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0nJZtNoE02X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getExif(image=None, key=None):\n",
        "    \"\"\"\n",
        "    Returns a tuple of exif key and exif value\n",
        "    \"\"\"\n",
        "    for (k, v) in PIL.ExifTags.TAGS.items():\n",
        "        if v.lower() == key:\n",
        "            #print(v.lower(), \" \", key)\n",
        "            if image._getexif():\n",
        "                return (k, image._getexif()[k])\n",
        "            else:\n",
        "                return (k, 1)\n",
        "    \n",
        "def fixOrientation(image=None, orientation=1):\n",
        "\n",
        "    result = image\n",
        "    #print(orientation)\n",
        "    if orientation == 3 : \n",
        "        result =  image.rotate(180, expand=True)\n",
        "    elif orientation == 6 : \n",
        "        result = image.rotate(270, expand=True)\n",
        "    elif orientation == 8 : \n",
        "        result = image.rotate(90, expand=True)\n",
        "        \n",
        "    return result\n",
        "\n",
        "def isValidImage(filename=None):\n",
        "    BLACKLIST = [\"kaztest.jpg\"]\n",
        "    VALID_EXTENSIONS = [\"jpg\", \"png\"]\n",
        "    name, extension = filename.split(\".\")\n",
        "    \n",
        "    if extension.lower() not in VALID_EXTENSIONS:\n",
        "        return False\n",
        "    \n",
        "    if filename in BLACKLIST:\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "                        \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBbV9Tora90P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skimage\n",
        "import numpy as np\n",
        "\n",
        "def color_splash(image, mask):\n",
        "    \"\"\"Apply color splash effect.\n",
        "    image: RGB image [height, width, 3]\n",
        "    mask: instance segmentation mask [height, width, instance count]\n",
        "    Returns result image.\n",
        "    \"\"\"\n",
        "    # Make a grayscale copy of the image. The grayscale copy still\n",
        "    # has 3 RGB channels, though.\n",
        "    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n",
        "    # Copy color pixels from the original color image where mask is set\n",
        "    if mask.shape[-1] > 0:\n",
        "        # We're treating all instances as one, so collapse the mask into one layer\n",
        "        mask = (np.sum(mask, -1, keepdims=True) >= 1)\n",
        "        splash = np.where(mask, image, gray).astype(np.uint8)\n",
        "    else:\n",
        "        splash = gray.astype(np.uint8)\n",
        "    return splash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uE2h-mZc3MJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from mrcnn.config import Config\n",
        "\n",
        "# Derived from https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb\n",
        "class CocoConfig(Config):\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"coco\"\n",
        "    \n",
        "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
        "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    \n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 80\n",
        "    \n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 128\n",
        "    IMAGE_MAX_DIM = 128\n",
        "    \n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    RPM_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Reduce training ROIs per image because the images are small and have\n",
        "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "    \n",
        "    # Use a small epoch since the data is simple\n",
        "    STEPS_PER_EPOCH = 100\n",
        "    \n",
        "    # use small validation steps since the epoch is small\n",
        "    VALIDATION_STEPS = 5\n",
        "    \n",
        "cocoConfig = CocoConfig()\n",
        "#cocoConfig.display()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJNOAQTYDgu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Coco Inference Config\n",
        "\n",
        "class InferenceCocoConfig(CocoConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    \n",
        "\n",
        "inferenceCocoConfig = InferenceCocoConfig()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f25S69brw8Xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Run inference on images \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "import numpy\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import PIL.ExifTags\n",
        "from mrcnn import model as modellib, utils\n",
        "from PIL import Image\n",
        "\n",
        "# Directory to save logs and model checkpoints, if not provided\n",
        "# through the command line argument --logs\n",
        "DEFAULT_LOGS_DIR = \"/root/LOGS/\"\n",
        "\n",
        "model = modellib.MaskRCNN(mode=\"inference\", config=inferenceCocoConfig,\n",
        "                                  model_dir=DEFAULT_LOGS_DIR)\n",
        "\n",
        "weights_path = WEIGHTS_FILE\n",
        "\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "    \n",
        "    \n",
        "RESULTS = []\n",
        "\n",
        "for imageFilename in os.listdir(imageFilesBasePath)[20:30]:\n",
        "    if not isValidImage(imageFilename):\n",
        "        continue\n",
        "    \n",
        "    imagePath = os.path.join(imageFilesBasePath, imageFilename)\n",
        "    maskPath = getImageFromDir(maskFilesBasePath, imageFilename, \"macbeth\")\n",
        "\n",
        "    if not maskPath:\n",
        "        continue\n",
        "    \n",
        "    _image = skimage.io.imread(imagePath)    \n",
        "    _resizeResult = utils.resize_image(_image, max_dim=1024)\n",
        "    _scale = _resizeResult[2]\n",
        "    _padding = _resizeResult[3]\n",
        "    _crop = _resizeResult[4]\n",
        "    imageSrc = _resizeResult[0]\n",
        "    #imageSrc = utils.resize_image(Image.open(imagePath),max_dim=1024)\n",
        "    _maskSrc = skimage.io.imread(maskPath)  #Image.open(maskPath)\n",
        "    \n",
        "    print(_maskSrc.shape)\n",
        "    print(_image.shape)\n",
        "    \n",
        "    _maskTmp = np.expand_dims(_maskSrc, axis=2)\n",
        "    _maskTmp2 = np.tile(_maskTmp, 3)\n",
        "    #_maskTmp = np.tile(_maskSrc, 3)\n",
        "    print (_maskTmp2.shape)\n",
        "    \n",
        "    print(\"####\")\n",
        "    #print(_image.shape)\n",
        "    #print(_maskSrc.shape)\n",
        "    print(_scale, _padding, _crop)\n",
        "    print(_maskSrc)\n",
        "    maskSrc = utils.resize_mask(_maskTmp2, _scale, _padding, _crop)\n",
        "    print(maskSrc.shape)\n",
        "\n",
        "    #orientation = getExif(imageSrc, \"orientation\")\n",
        "    #imageOrientated = fixOrientation(imageSrc, orientation[1])\n",
        "    image = Image.fromarray(imageSrc)\n",
        "    maskImage = Image.fromarray(maskSrc)\n",
        "    \n",
        "    # Test overlays of masks\n",
        "    channels = image.split()\n",
        "    channelsMask = maskImage.split()\n",
        "    \n",
        "    testImage = PIL.ImageChops.add(channels[0], channelsMask[0])\n",
        "    newImage = Image.merge(\"RGB\", (testImage, channels[1], channels[2]))\n",
        "        \n",
        "    imageArray = numpy.array(imageSrc)\n",
        "\n",
        "    \n",
        "    #plt.figure(figsize=(8, 6))\n",
        "    #plt.title(imagePath.split(\"/\").pop())\n",
        "    #plt.imshow(newImage)\n",
        "    \n",
        "    result = model.detect([imageArray], verbose=1)[0]\n",
        "    RESULTS.append(result)\n",
        "    splash = color_splash(imageArray, result[\"masks\"])\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(result[\"class_ids\"])\n",
        "    plt.imshow(splash)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U35sKd8mUUnv",
        "colab_type": "code",
        "outputId": "35858338-664c-4d79-8f1b-189427345258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "# Macbeth Config\n",
        "\n",
        "from mrcnn.config import Config\n",
        "\n",
        "# Derived from https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb\n",
        "class MacbethConfig(Config):\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"macbeth\"\n",
        "    \n",
        "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
        "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    \n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1\n",
        "    \n",
        "    # Use small images for faster training. Set the limits of the small side\n",
        "    # the large side, and that determines the image shape.\n",
        "    IMAGE_MIN_DIM = 128\n",
        "    IMAGE_MAX_DIM = 128\n",
        "    \n",
        "    # Use smaller anchors because our image and objects are small\n",
        "    RPM_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "    \n",
        "    # Reduce training ROIs per image because the images are small and have\n",
        "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "    \n",
        "    # Use a small epoch since the data is simple\n",
        "    STEPS_PER_EPOCH = 100\n",
        "    \n",
        "    # use small validation steps since the epoch is small\n",
        "    VALIDATION_STEPS = 5\n",
        "    \n",
        "config = MacbethConfig()\n",
        "config.display()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  128\n",
            "IMAGE_META_SIZE                14\n",
            "IMAGE_MIN_DIM                  128\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [128 128   3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           macbeth\n",
            "NUM_CLASSES                    2\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPM_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                100\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           32\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               5\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtBuVhcbQckU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def my_resize_image(image, min_dim=None, max_dim=None, min_scale=None, mode=\"square\"):\n",
        "    \"\"\"Resizes an image keeping the aspect ratio unchanged.\n",
        "\n",
        "    min_dim: if provided, resizes the image such that it's smaller\n",
        "        dimension == min_dim\n",
        "    max_dim: if provided, ensures that the image longest side doesn't\n",
        "        exceed this value.\n",
        "    min_scale: if provided, ensure that the image is scaled up by at least\n",
        "        this percent even if min_dim doesn't require it.\n",
        "    mode: Resizing mode.\n",
        "        none: No resizing. Return the image unchanged.\n",
        "        square: Resize and pad with zeros to get a square image\n",
        "            of size [max_dim, max_dim].\n",
        "        pad64: Pads width and height with zeros to make them multiples of 64.\n",
        "               If min_dim or min_scale are provided, it scales the image up\n",
        "               before padding. max_dim is ignored in this mode.\n",
        "               The multiple of 64 is needed to ensure smooth scaling of feature\n",
        "               maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
        "        crop: Picks random crops from the image. First, scales the image based\n",
        "              on min_dim and min_scale, then picks a random crop of\n",
        "              size min_dim x min_dim. Can be used in training only.\n",
        "              max_dim is not used in this mode.\n",
        "\n",
        "    Returns:\n",
        "    image: the resized image\n",
        "    window: (y1, x1, y2, x2). If max_dim is provided, padding might\n",
        "        be inserted in the returned image. If so, this window is the\n",
        "        coordinates of the image part of the full image (excluding\n",
        "        the padding). The x2, y2 pixels are not included.\n",
        "    scale: The scale factor used to resize the image\n",
        "    padding: Padding added to the image [(top, bottom), (left, right), (0, 0)]\n",
        "    \"\"\"\n",
        "    # Keep track of image dtype and return results in the same dtype\n",
        "    image_dtype = image.dtype\n",
        "    # Default window (y1, x1, y2, x2) and default scale == 1.\n",
        "    h, w = image.shape[:2]\n",
        "    window = (0, 0, h, w)\n",
        "    scale = 1\n",
        "    padding = [(0, 0), (0, 0), (0, 0)]\n",
        "    crop = None\n",
        "\n",
        "    if mode == \"none\":\n",
        "        return image, window, scale, padding, crop\n",
        "\n",
        "    # Scale?\n",
        "    if min_dim:\n",
        "        # Scale up but not down\n",
        "        scale = max(1, min_dim / min(h, w))\n",
        "    if min_scale and scale < min_scale:\n",
        "        scale = min_scale\n",
        "\n",
        "    # Does it exceed max dim?\n",
        "    if max_dim and mode == \"square\":\n",
        "        image_max = max(h, w)\n",
        "        if round(image_max * scale) > max_dim:\n",
        "            scale = max_dim / image_max\n",
        "\n",
        "    # Resize image using bilinear interpolation\n",
        "    if scale != 1:\n",
        "        image = utils.resize(image, (round(h * scale), round(w * scale)),\n",
        "                       preserve_range=True)\n",
        "\n",
        "    # Need padding or cropping?\n",
        "    if mode == \"square\":\n",
        "        # Get new height and width\n",
        "        h, w = image.shape[:2]\n",
        "        top_pad = (max_dim - h) // 2\n",
        "        bottom_pad = max_dim - h - top_pad\n",
        "        left_pad = (max_dim - w) // 2\n",
        "        right_pad = max_dim - w - left_pad\n",
        "        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\n",
        "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
        "        window = (top_pad, left_pad, h + top_pad, w + left_pad)\n",
        "    elif mode == \"pad64\":\n",
        "        h, w = image.shape[:2]\n",
        "        # Both sides must be divisible by 64\n",
        "        assert min_dim % 64 == 0, \"Minimum dimension must be a multiple of 64\"\n",
        "        # Height\n",
        "        if h % 64 > 0:\n",
        "            max_h = h - (h % 64) + 64\n",
        "            top_pad = (max_h - h) // 2\n",
        "            bottom_pad = max_h - h - top_pad\n",
        "        else:\n",
        "            top_pad = bottom_pad = 0\n",
        "        # Width\n",
        "        if w % 64 > 0:\n",
        "            max_w = w - (w % 64) + 64\n",
        "            left_pad = (max_w - w) // 2\n",
        "            right_pad = max_w - w - left_pad\n",
        "        else:\n",
        "            left_pad = right_pad = 0\n",
        "        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\n",
        "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
        "        window = (top_pad, left_pad, h + top_pad, w + left_pad)\n",
        "    elif mode == \"crop\":\n",
        "        # Pick a random crop\n",
        "        h, w = image.shape[:2]\n",
        "        y = random.randint(0, (h - min_dim))\n",
        "        x = random.randint(0, (w - min_dim))\n",
        "        crop = (y, x, min_dim, min_dim)\n",
        "        image = image[y:y + min_dim, x:x + min_dim]\n",
        "        window = (0, 0, min_dim, min_dim)\n",
        "    else:\n",
        "        raise Exception(\"Mode {} not supported\".format(mode))\n",
        "    return image.astype(image_dtype), window, scale, padding, crop\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCSAPTkbX5mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import datetime\n",
        "import numpy as np\n",
        "import skimage.draw\n",
        "\n",
        "from mrcnn import model as modellib, utils\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(BASEDIR)\n",
        "\n",
        "# URL from which to download the latest COCO trained weights\n",
        "COCO_MODEL_URL = \"https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\"\n",
        "\n",
        "# Directory to save logs and model checkpoints, if not provided\n",
        "# through the command line argument --logs\n",
        "DEFAULT_LOGS_DIR = \"/root/LOGS/\"\n",
        "\n",
        "#imageFilesBasePath = \"/root/data/mine/machineLearning/macbethIdentify/data/data/img\"\n",
        "maskFilesBasePath = maskFilesBasePath\n",
        "\n",
        "\n",
        "# Dataset\n",
        "class MacbethDataset(utils.Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.imageResizeParams = {}\n",
        "    \n",
        "    def load_macbeths(self, dataset_dir, subset):\n",
        "        # Add classes. We have only one class to add.\n",
        "        self.add_class(\"macbeth\", 1, \"macbeth\")\n",
        "        \n",
        "        i = 0\n",
        "\n",
        "        for imageFilename in os.listdir(dataset_dir)[:5]:\n",
        "            if not isValidImage(imageFilename):\n",
        "                continue\n",
        "    \n",
        "            imagePath = os.path.join(dataset_dir, imageFilename)\n",
        "            maskPath = getImageFromDir(maskFilesBasePath, imageFilename, \"macbeth\")  \n",
        "            \n",
        "            if not maskPath:\n",
        "                continue\n",
        "                \n",
        "            if not os.path.isfile(imagePath):\n",
        "                continue\n",
        "                \n",
        "            if not os.path.isfile(maskPath):\n",
        "                continue\n",
        "                \n",
        "\n",
        "            _image = skimage.io.imread(imagePath)\n",
        "            #print(_image.shape)\n",
        "            _resizeResults = my_resize_image(_image, max_dim=1024)\n",
        "            #print(\"_resizeResults: \", _resizeResults)\n",
        "            image = _resizeResults[0]\n",
        "            _scale = _resizeResults[2]\n",
        "            _padding = _resizeResults[3]\n",
        "            _crop = _resizeResults[4]\n",
        "\n",
        "            #print(\"_scale: \", _scale, \"_padding: \", _padding)\n",
        "            self.imageResizeParams[i] = [_scale, _padding, _crop]\n",
        "            height, width = image.shape[:2]\n",
        "            #print(width, height)\n",
        "            #print(\"Adding: \", imagePath)\n",
        "            #print(\"Adding: \", maskPath)\n",
        "            self.add_image(\n",
        "                \"macbeth\",\n",
        "                image_id=i,\n",
        "                path=imagePath,\n",
        "                width=width,\n",
        "                height=height\n",
        "            )\n",
        "            self._image_ids.append(i)\n",
        "            i += 1\n",
        "        \n",
        "        \n",
        "    def load_mask(self, image_id):\n",
        "        \n",
        "        maskPath = getImageFromDir(maskFilesBasePath, self.image_info[image_id][\"path\"].split(\"/\").pop(), \"macbeth\")  \n",
        "        #print(maskPath)\n",
        "        assert os.path.isfile(maskPath)\n",
        "\n",
        "        # I think this is width, height, number of instances\n",
        "        #mask = np.zeros([self.image_info[image_id][\"width\"], self.image_info[image_id][\"height\"], 1], dtype=np.uint8)\n",
        "        _maskSrc = skimage.io.imread(maskPath)\n",
        "        \n",
        "        print(\"_maskSrc.shape: \", _maskSrc.shape)\n",
        "        _maskTmp = np.expand_dims(_maskSrc, axis=2)\n",
        "        print(\"_maskTmp.shape: \", _maskTmp.shape)\n",
        "        #_maskTmp2 = np.tile(_maskTmp, 3)\n",
        "        #_maskTmp = np.tile(_maskSrc, 3)\n",
        "        #print (_maskTmp2.shape)\n",
        "\n",
        "        _scale = self.imageResizeParams[image_id][0]\n",
        "        _padding = self.imageResizeParams[image_id][1]\n",
        "        print(\"_scale: \", _scale, \"_padding: \", _padding)\n",
        "        \n",
        "        maskSrc = utils.resize_mask(_maskTmp, _scale, _padding)\n",
        "        #maskArray = numpy.array(maskSrc)\n",
        "        \n",
        "        #print(\"tile: \", _maskTmp2.shape)\n",
        "        #print(\"msh:\", mask.shape)\n",
        "        print(\"maskSrc.shape:\", maskSrc.shape)\n",
        "        \n",
        "        #return [1,_maskTmp]\n",
        "        mask = np.zeros([self.image_info[image_id][\"width\"], self.image_info[image_id][\"height\"], 1], dtype=np.uint8)\n",
        "        print([self.image_info[image_id][\"width\"], self.image_info[image_id][\"height\"]])\n",
        "        print(\"mask.shape: \", mask.shape)\n",
        "        return mask\n",
        "    \n",
        "    def image_reference(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"macbeth\":\n",
        "            return info[\"path\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcb-VNdcyyYW",
        "colab_type": "code",
        "outputId": "e2cf254c-6683-446a-b7d2-9e59d45ef538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(BASEDIR)\n",
        "print(maskFilesBasePath)\n",
        "print(imageFilesBasePath)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1//data/mask_images\n",
            "/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1//data/img\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CPQcbYWaPUl",
        "colab_type": "code",
        "outputId": "d40353ac-fe20-4333-f5d2-e4fe5e652f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# test stuff\n",
        "imgPath = \"/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1//data/img/penn_jillette_00135.jpg\"\n",
        "maskPath = \"/content/drive/My Drive/MachineLearning/projects/macbethIdentify_v1//data/mask_images/penn_jillette_00135_class_macbeth.png\"\n",
        "#mask = np.zeros([1000, 2000, 1], dtype=np.uint8)\n",
        "\n",
        "#test = MacbethDataset()\n",
        "#test.load_macbeths(imageFilesBasePath, \"train\")\n",
        "\n",
        "#array = test.load_image(1)\n",
        "\n",
        "#print(type(array))\n",
        "#print(type(mask))\n",
        "\n",
        "#print(array.shape)\n",
        "\n",
        "image = skimage.io.imread(imgPath)\n",
        "_mask = skimage.io.imread(maskPath)\n",
        "\n",
        "mask = numpy.expand_dims(_mask, axis=2)\n",
        "\n",
        "print(image.shape)\n",
        "print(_mask.shape)\n",
        "print(mask.shape)\n",
        "\n",
        "resized_image = utils.resize_image(image, max_dim=1024)\n",
        "resized_mask = utils.resize_mask(mask, resized_image[2], resized_image[3])\n",
        "\n",
        "print(resized_image[0].shape)\n",
        "print(resized_mask.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1869, 1400, 3)\n",
            "(1869, 1400)\n",
            "(1869, 1400, 1)\n",
            "(1024, 1024, 3)\n",
            "(1024, 1024, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1K36Et6c6BI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2672
        },
        "outputId": "6460ab0e-49ea-4906-eb6b-040d5607081a"
      },
      "source": [
        "#\n",
        "# Visualise image and matte overlays are working through the dataset class\n",
        "#\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "import numpy\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import PIL.ExifTags\n",
        "from mrcnn import model as modellib, utils\n",
        "\n",
        "#import importlib\n",
        "#importlib.reload(mrcnn.utils)\n",
        "\n",
        "dataset_train = MacbethDataset()\n",
        "dataset_train.load_macbeths(imageFilesBasePath, \"train\")\n",
        "\n",
        "\n",
        "\n",
        "for i, f in enumerate(dataset_train.image_ids):\n",
        "    \n",
        "    #_img = dataset_train.load_image\n",
        "    _img = dataset_train.load_image(f)\n",
        "    print(_img.shape)\n",
        "    \n",
        "    _resizeResults = my_resize_image(_img, max_dim=1024)\n",
        "    img = Image.fromarray(_resizeResults[0])\n",
        "    \n",
        "    #tst = dataset_train.load_image(f)\n",
        "    #print(tst.dtype)\n",
        "    #img2 = utils.resize_image(img, max_dim=1024)\n",
        "    \n",
        "    mask = Image.fromarray(dataset_train.load_mask(f)[1])\n",
        "    \n",
        "    print(\"img.size: \", img.size)\n",
        "    print(\"width: \", mask.width)\n",
        "    print(\"height:\", mask.height)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    #plt.title(\"banana\")\n",
        "    plt.imshow(mask)\n",
        "    \n",
        "    \"\"\"\n",
        "    # Test overlays of masks\n",
        "    channels = img.split()\n",
        "    #print(channels)\n",
        "    #print(mask)\n",
        "    \n",
        "    testImage = PIL.ImageChops.add(channels[0], mask)\n",
        "    newImage = Image.merge(\"RGB\", (testImage, channels[1], channels[2]))\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(\"banana\")\n",
        "    plt.imshow(newImage)\n",
        "\"\"\"    \n",
        "    \n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1869, 1400, 3)\n",
            "_maskSrc.shape:  (1869, 1400)\n",
            "_maskTmp.shape:  (1869, 1400, 1)\n",
            "_scale:  0.5478865703584804 _padding:  [(0, 0), (128, 129), (0, 0)]\n",
            "maskSrc.shape: (1024, 1024, 1)\n",
            "[1024, 1024]\n",
            "mask.shape:  (1024, 1024, 1)\n",
            "img.size:  (1024, 1024)\n",
            "width:  1\n",
            "height: 1024\n",
            "(1869, 1400, 3)\n",
            "_maskSrc.shape:  (1869, 1400)\n",
            "_maskTmp.shape:  (1869, 1400, 1)\n",
            "_scale:  0.5478865703584804 _padding:  [(0, 0), (128, 129), (0, 0)]\n",
            "maskSrc.shape: (1024, 1024, 1)\n",
            "[1024, 1024]\n",
            "mask.shape:  (1024, 1024, 1)\n",
            "img.size:  (1024, 1024)\n",
            "width:  1\n",
            "height: 1024\n",
            "(2670, 2000, 3)\n",
            "_maskSrc.shape:  (2670, 2000)\n",
            "_maskTmp.shape:  (2670, 2000, 1)\n",
            "_scale:  0.38352059925093634 _padding:  [(0, 0), (128, 129), (0, 0)]\n",
            "maskSrc.shape: (1024, 1024, 1)\n",
            "[1024, 1024]\n",
            "mask.shape:  (1024, 1024, 1)\n",
            "img.size:  (1024, 1024)\n",
            "width:  1\n",
            "height: 1024\n",
            "(2671, 2000, 3)\n",
            "_maskSrc.shape:  (2671, 2000)\n",
            "_maskTmp.shape:  (2671, 2000, 1)\n",
            "_scale:  0.3833770123549233 _padding:  [(0, 0), (128, 129), (0, 0)]\n",
            "maskSrc.shape: (1024, 1024, 1)\n",
            "[1024, 1024]\n",
            "mask.shape:  (1024, 1024, 1)\n",
            "img.size:  (1024, 1024)\n",
            "width:  1\n",
            "height: 1024\n",
            "(1512, 1008, 3)\n",
            "_maskSrc.shape:  (1512, 1008)\n",
            "_maskTmp.shape:  (1512, 1008, 1)\n",
            "_scale:  0.6772486772486772 _padding:  [(0, 0), (170, 171), (0, 0)]\n",
            "maskSrc.shape: (1024, 1024, 1)\n",
            "[1024, 1024]\n",
            "mask.shape:  (1024, 1024, 1)\n",
            "img.size:  (1024, 1024)\n",
            "width:  1\n",
            "height: 1024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAFpCAYAAADN1jvFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAFpCAYAAADN1jvFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAFpCAYAAADN1jvFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAFpCAYAAADN1jvFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADoAAAFpCAYAAADN1jvFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJX9GlLlfikA",
        "colab_type": "code",
        "outputId": "bf5db0a0-dd99-492d-c329-a09157d98810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "#\n",
        "# Perform Training\n",
        "#\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=DEFAULT_LOGS_DIR)\n",
        "model.load_weights(WEIGHTS_FILE)\n",
        "print(\"FSFDFDF\")\n",
        "\n",
        "dataset_train = MacbethDataset()\n",
        "dataset_train.load_macbeths(imageFilesBasePath, \"train\")\n",
        "dataset_train.prepare()\n",
        "\n",
        "#dataset_val = MacbethDataset()\n",
        "#dataset_val.load_macbeths(\"/root/data/mine/machineLearning/macbethIdentify/data/data/img_val\", \"val\")\n",
        "#dataset_val.prepare()\n",
        "\n",
        "print(\"Training network heads\")\n",
        "model.train(dataset_train, dataset_val,\n",
        "           learning_rate=config.LEARNING_RATE,\n",
        "           epochs=30,\n",
        "           layers='heads')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1658\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 1 in both shapes must be equal, but are 8 and 324. Shapes are [1024,8] and [1024,324]. for 'Assign_2738' (op: 'Assign') with input shapes: [1024,8], [1024,324].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-ab96180e5a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_LOGS_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWEIGHTS_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FSFDFDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMacbethDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2132\u001b[0;31m             \u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                              ' elements.')\n\u001b[1;32m   1057\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2463\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2464\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2465\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \"\"\"\n\u001b[1;32m   1761\u001b[0m     assign = state_ops.assign(self._variable, value, use_locking=use_locking,\n\u001b[0;32m-> 1762\u001b[0;31m                               name=name)\n\u001b[0m\u001b[1;32m   1763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    221\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    222\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    224\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     62\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                   use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3298\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3299\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3300\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3301\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1821\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1822\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1823\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimension 1 in both shapes must be equal, but are 8 and 324. Shapes are [1024,8] and [1024,324]. for 'Assign_2738' (op: 'Assign') with input shapes: [1024,8], [1024,324]."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMIy8eWoiJLN",
        "colab_type": "code",
        "outputId": "ea28761c-0dcb-4ee2-c99c-ebb939ec9489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "%cd ~/data//mine/machineLearning/macbethIdentify/data/data/\n",
        "%ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/data/mine/machineLearning/macbethIdentify/data/data\n",
            "total 48\n",
            "drwx------ 2 root root  4096 May 26 05:05 \u001b[0m\u001b[01;34mannotations\u001b[0m/\n",
            "drwx------ 2 root root  4096 May 26 05:05 \u001b[01;34mannottions_json\u001b[0m/\n",
            "drwx------ 2 root root  4096 May 26 05:05 \u001b[01;34mimg\u001b[0m/\n",
            "drwx------ 2 root root  4096 May 26 04:59 \u001b[01;34mimg_val\u001b[0m/\n",
            "-rwx------ 1 root root    34 Apr 27 03:50 \u001b[01;32mlabel_map.pbtxt\u001b[0m*\n",
            "drwx------ 2 root root 16384 May 26 05:05 \u001b[01;34mmask_images\u001b[0m/\n",
            "-rwx------ 1 root root   127 Apr  9 00:21 \u001b[01;32mtest.txt\u001b[0m*\n",
            "-rwx------ 1 root root  1031 Apr 27 03:50 \u001b[01;32mtrain.txt\u001b[0m*\n",
            "-rwx------ 1 root root   141 Apr  9 00:21 \u001b[01;32mval.txt\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd4KUgaZMLl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}